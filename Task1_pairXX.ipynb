{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc4ba4e",
   "metadata": {},
   "source": [
    "# CS 677 Lab 4 - AI/ML (and Semi-Cloud)\n",
    "\n",
    "Welcome to the final lab of the course! In this lab, we will explore deploying an AI model on an edge device. You'll notice that training models for embedded edge devices is quite similar to training traditional models, but with a few additional parameters and considerations.\n",
    "\n",
    "For this lab, you will create a Fitbit-like activity recognition band that utilizes a neural network (NN) along with an accelerometer and gyroscope to classify various activities. We will use TensorFlow Lite to develop our models.\n",
    "Considerations for Edge ML:\n",
    "\n",
    "- Models need to be small and optimized for limited resources.\n",
    "- Not all edge devices have floating-point (FP) arithmetic logic units (ALUs).\n",
    "- Input data storage must be efficiently managed due to space constraints.\n",
    "\n",
    "Finally, we will integrate ThingsBoard to create a dashboard for your Fitbit-like activity tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce500b-581e-4d22-a9fd-7a6990619b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39ee5c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "You are provied a modifled version of a Human Activity Recognition Dataset (We added some standing position data to the dataset). This dataset contains data from **33 volunteers** with demographic attributes including **gender**, **height**, **weight**, and **age**. Volunteers wore the wristband named **BandX** on their dominant hand while performing specific activities. The **MPU-6050** sensor module captured data from the **accelerometer** and **gyroscope**. This dataset captures human activity data for the following **seven types of activities**:  \n",
    "\n",
    "1. **Walking** (Wa)  \n",
    "2. **Jogging** (J)  \n",
    "3. **Typing** (T)  \n",
    "4. **Writing** (Wr)  \n",
    "5. **Upstairs movement** (U)  \n",
    "6. **Downstairs movement** (D)  \n",
    "7. **Cycling** (C) \n",
    "\n",
    "We then modified the dataset and added a few samples of **Standing** (S), and removed **Upstairs movement** (U) and **Downstairs movement** (D)   for better working of the application.\n",
    "The sensor data includes Accelerometer (`ax`, `ay`, `az`) and Gyroscope (`gx`, `gy`, `gz`) data collected using the **MPU-6050** sensor, sampling at **20 Hz**.\n",
    "\n",
    "The processed dataset has been created by take **2 second samples** of an activity (2 second sample at 20Hz means that each input tensor is of 40 readings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08121d8",
   "metadata": {},
   "source": [
    "## Task 1.1\n",
    "Load the dataset using NumPy and extract the train, validation and test sets. The given file is in NPZ format, so you can extract the data using its keys. (Print the loaded data temporarily for clarity).\n",
    "\n",
    "After extracting the sets, ensure that all the data in the feature sets are of type `float32`. Additionally, one-hot encode the target sets. We advise using sklearn's `LabelEncoder` and ` to_categorical` from `keras.utils`, but its up to you.\n",
    "\n",
    "The dataset has already been normalized and filtered for you.\n",
    "\n",
    "Also, make a note of how the encodings correspond to specific activities (i.e., which encoding represents which activity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b513a5c-bbc2-4b31-a8b6-0057be12dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "# Your code here\n",
    "\n",
    "# Ensure data is numerical\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "\n",
    "# Make a note of the class the class\n",
    "print(\"Classes and their encoding:\")\n",
    "for i, label in enumerate(encoder.classes_):\n",
    "    print(f\"{label} -> {i}\")\n",
    "\n",
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772acae6",
   "metadata": {},
   "source": [
    "Now that we have loaded our dataset, let's design and train our model. We will be training a **Neural Network** for our use case using the **TensorFlow Keras** library to construct the layers of our model. \n",
    "\n",
    "The `keras.layers` module provides a variety of different layers, which you can explore in the [official documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, LSTM, SeparableConv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509d93c",
   "metadata": {},
   "source": [
    "## Task 1.2 Creating and Training NN\n",
    "Select a sequence of layers for your neural network. Use your prior knowledge of NN architectures to construct what you believe would be a suitable model.  \n",
    "\n",
    "The `keras.layers` library offers many different layers to choose from, so feel free to experiment with different sequences.  \n",
    "\n",
    "*(For example, in my attempt, I used `Conv1D`, `MaxPooling1D`, `Flatten`, `Dense`, `Dropout`, and `BatchNormalization`, but you are free to try different configurations!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069e597-8d85-472a-9934-684255fc7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Architecture\n",
    "model = Sequential([\n",
    "    # Example of how to define the layers. Change these out with your own set of layers.\n",
    "    Flatten(input_shape=(x_train.shape[1], x_train.shape[2])),  # Flatten (40,6) to 240\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc14bed",
   "metadata": {},
   "source": [
    "Using the Keras Sequential model defined above, proceed with the following steps:  \n",
    "\n",
    "1. Compile the model:  \n",
    "   - Choose an appropriate **optimizer** (e.g., `Adam` or `SGD`).  \n",
    "   - Use an appropriate **loss function** (e.g., `categorical_crossentropy` for multi-class classification).  \n",
    "   - Select **metrics** to evaluate the model’s performance (e.g., `accuracy`).  \n",
    "\n",
    "2. **Train** the model:  \n",
    "   - Use the `.fit()` function with training data.  \n",
    "   - Set an appropriate number of **epochs** and **batch size**.  \n",
    "   - Use **validation data** to monitor performance.  \n",
    "\n",
    "3. **Evaluate** the model:  \n",
    "   - Use the `.evaluate()` function on the test set to measure final accuracy and loss.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=, loss=, metrics=[])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {eval_results[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff736a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d1ee0",
   "metadata": {},
   "source": [
    "In this section, evaluate your model’s classification report (the code for this has already been provided). Is the precision and recall for all target classes above the required threshold (90%)? Is the overall accuracy over 95%? If not, can you make adjustments can we make to improve the model’s performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7339f-e02d-4a2f-8f64-9dbc1bddab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score\n",
    "\n",
    "def evaluate_model(model, x_test, y_test, encoder=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and compute precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model.\n",
    "    - x_test: Test features.\n",
    "    - y_test: True labels for test data (categorical or one-hot encoded).\n",
    "    - encoder: LabelEncoder instance for decoding labels (optional).\n",
    "\n",
    "    Returns:\n",
    "    - Classification report with precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # If y_test is one-hot encoded, decode it\n",
    "    if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, x_test, y_test, encoder=encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374eca4",
   "metadata": {},
   "source": [
    "## Task 1.3 TF Lite\n",
    "\n",
    "Now that you have created, trained, and saved your model, let's convert it into a **TensorFlow Lite (.tflite)** format. TensorFlow Lite is a lightweight version of TensorFlow designed for running machine learning models on edge devices such as microcontrollers, mobile phones, and IoT devices. It enables fast inference with low power consumption and reduced memory footprint.\n",
    "\n",
    "The following block convert the trained model into a .tflite file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202da79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"trained_model.h5\")\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a .tflite file\n",
    "with open(\"unquantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf677f",
   "metadata": {},
   "source": [
    "One of the most powerful features of TensorFlow Lite (TFLite) is its ability to perform **quantization**. Quantization significantly reduces the **memory footprint** of the model and decreases the **computational cycles** required for inference, making it highly efficient for deployment on edge devices.  \n",
    "\n",
    "However, quantization comes with a trade-off. The process introduces a certain level of **information loss**, which can negatively impact the model's accuracy. \n",
    "\n",
    "In this task, your goal is to **quantize the model** so that its size remains **below 100 KB** (size of .h file). When converting a TensorFlow model to TFLite, additional parameters and conditions can be applied to the **converter object** to modify the model. The TensorFlow Lite **TFLiteConverter API** provides various options for optimizing model size and performance. You can explore the available attributes and methods [here](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter).  \n",
    "\n",
    "One of the most useful attributes is **`target_spec`**, which allows control over the **data types** used in computations. By leveraging this, you can convert certain layers from floating-point precision to integer values, significantly reducing model size while maintaining acceptable accuracy. More details on this process can be found in [this guide](https://www.tensorflow.org/api_docs/python/tf/lite/TargetSpec).  \n",
    "\n",
    "Your task is to experiment with **different quantization techniques** and apply modifications to ensure the model remains within the required size constraints while retaining as much accuracy as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c703ed-cded-40a2-a72c-500f3b2e905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"trained_model.h5\")\n",
    "\n",
    "# Convert to fully quantized TFLite model (int8)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Your code here (Addional method calls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "converter._experimental_disable_per_channel_quantization = True\n",
    "converter._experimental_disable_per_channel_quantization_for_dense_layers = True\n",
    "\n",
    "# Convert and save\n",
    "tflite_model = converter.convert()\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438a3fd",
   "metadata": {},
   "source": [
    "To run your model on your esp you need to **convert** the tflite into a **.h file** using ``xxd -i model.tflite > model.h`` on unix systems or by using the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47273f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .tflite model\n",
    "with open(\"quantized_model.tflite\", \"rb\") as f:\n",
    "    model_bytes = f.read()\n",
    "\n",
    "# Convert to C array format\n",
    "c_array = \", \".join(f\"0x{b:02x}\" for b in model_bytes)\n",
    "\n",
    "# Write to .h file\n",
    "with open(\"model.h\", \"w\") as f:\n",
    "    f.write(\"#ifndef MODEL_H\\n#define MODEL_H\\n\\n\")\n",
    "    f.write(f\"const unsigned char model_tflite[] = {{ {c_array} }};\\n\")\n",
    "    f.write(f\"const unsigned int model_tflite_len = {len(model_bytes)};\\n\")\n",
    "    f.write(\"#endif // MODEL_H\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0ed30",
   "metadata": {},
   "source": [
    "Here is some code to evaluate the accuracy of your quntized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c32ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load test data (assuming X_test and y_test are already preprocessed)\n",
    "x_test = x_test.astype(np.float32)  # Ensure correct data type\n",
    "y_true = np.argmax(y_test, axis=1)  # Convert one-hot to class indices\n",
    "\n",
    "# Run inference on test dataset\n",
    "y_pred = []\n",
    "for i in range(len(x_test)):\n",
    "    # Set the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], x_test[i:i+1])\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get output tensor and store prediction\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    y_pred.append(np.argmax(output_data))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"TFLite Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdda74a",
   "metadata": {},
   "source": [
    "***A concluding point***\n",
    "\n",
    "Why do we need such a small model? Well, the ESP32-S3 dev board that you have is quite featureful. It has 16 MB of flash, a powerful dual-core CPU, and a floating point unit. These are by no means guarantees in the IoT world. Additionally, utilizing all these resources to their fullest is quite expensive in terms of energy consumption.\n",
    "\n",
    "That's why it's really important to minimize the model's memory footprint as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49cac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.tflite\", \"rb\") as f:\n",
    "    model_content = f.read()\n",
    "\n",
    "tf.lite.experimental.Analyzer.analyze(model_content=model_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2b071",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-Lab",
   "language": "python",
   "name": "project-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
